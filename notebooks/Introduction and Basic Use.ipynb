{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from active_tester import ActiveTester\n",
    "from active_tester.estimators.naive import Naive\n",
    "from active_tester.query_strategy.random import Random\n",
    "from active_tester.estimators.learned import Learned\n",
    "from active_tester.label_estimation.methods import oracle_one_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Testing Overview\n",
    "\n",
    "The basic idea of active testing (see __[Active Testing: An Efficient and Robust Framework for Estimating Accuracy](https://icml.cc/Conferences/2018/Schedule?showEvent=2681)__) is to intelligently update the labels of a noisily-labeled test set to improve estimation of a performance metric for a particular system.  The noisy labels may come, for example, from crowdsourced workers and as a result are not expected to have high accuracy.  The required inputs to use active testing are:\n",
    "- A system under test\n",
    "- A performance metric (accuracy, precision, recall, etc.) of interest\n",
    "- A test dataset where each item has at least one noisy label and a score from the system under test\n",
    "- Access to a vetter that can provide (hopefully) high quality labels\n",
    "\n",
    "Active testing has two main steps.  In the first step, items from the test dataset are queried (according to some query_strategy) and sent to the vetter to receive a label.  In the second step, some combination of the system scores, the noisy labels, the vetted labels, and the features are used to estimate the performance metric of interest.\n",
    "\n",
    "This package implements a variety of query strategies and two metric estimation strategies.  Other notebooks will discuss these in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of basic use\n",
    "\n",
    "Running active testing in this package requires a couple of steps. \n",
    "\n",
    "First, we initialize the ActiveTester object, which requires us to set the `estimator` and `query_strategy` parameters, which require the user to pass an estimator object and query strategy object.  Here we use the `Naive` estimator and the `Random` query strategy.  The details of these objects are explained in other notebooks.  The only relevant point here is that the estimator object require us to specify the metric of interest.  Here we use accuracy.\n",
    "\n",
    "```python\n",
    "active_test = active_tester.ActiveTester(estimator=Naive(metric=accuracy_score), \n",
    "                                         query_strategy=Random())\n",
    "```\n",
    "\n",
    "Next, we call standardize data, which formats relevant data for the active testing routines.  Here, we need to set `X` the test dataset, `classes` the names of the classes, and `Y_noisy` the noisy labels from the experts.  Below is some additional info on the expected format for these parameters.  Later, we will discuss some additional parameters as well.\n",
    "* `X` should be a (number of items) x (number of features) array.\n",
    "* `classes` should be a list of strings that represent the class names\n",
    "* `Y_noisy` should be a (number of items) x (number of experts) array of the noisy labels.  If there is only 1 expert, note that the shape of the array must be (number of items) x 1.  If an expert does not provide a label for a particular item, a -1 should be used as a placeholder.\n",
    "\n",
    "```python\n",
    "active_test.standardize_data(X=x, \n",
    "                             classes=c, \n",
    "                             Y_noisy=y)\n",
    "```\n",
    "\n",
    "Finally, we call `gen_model_predictions`, and pass it the model we wish to evaluate.\n",
    "```python\n",
    "active_test.gen_model_predictions(model)\n",
    "```\n",
    "\n",
    "In the following cells, we construct the a training and test dataset, build a classifier, and construct some noisy labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X0 = np.random.randn(100,2)\n",
    "X1 = np.random.randn(100,2) + 2\n",
    "# Labels\n",
    "y0 = np.zeros(100)\n",
    "y1 = np.ones(100)\n",
    "# Stack together and split into train and test sets\n",
    "X = np.vstack((X0,X1))\n",
    "y = np.hstack((y0,y1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classifier and compute true performance on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression classifier \n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train,y_train)\n",
    "# Predict labels for the test set and compute the true accuracy of the classifier on the test set\n",
    "label_predictions = model.predict(X_test)\n",
    "true_accuracy = accuracy_score(y_test,label_predictions)\n",
    "print(true_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate noisy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_noisy = []\n",
    "noisy_label_accuracy = 0.75\n",
    "for i in range(len(y_test)):\n",
    "    if np.random.rand() < noisy_label_accuracy:\n",
    "        # noisy label is correct\n",
    "        y_noisy.append(y_test[i])\n",
    "    else:\n",
    "        # noisy label is incorrect\n",
    "        y_noisy.append(np.abs(1-y_test[i]))\n",
    "y_noisy = np.asarray(y_noisy, dtype=int)\n",
    "y_noisy = np.reshape(y_noisy,(len(y_noisy),1)) # Remember that this shape is important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup active testing\n",
    "\n",
    "Now that we've set up a simple dataset, we can initialize the `ActiveTester` object, and then call `standardize_data` and `gen_model_predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_test = ActiveTester(estimator=Naive(metric=accuracy_score), \n",
    "                                         query_strategy=Random())\n",
    "active_test.standardize_data(X=X_test, \n",
    "                             classes=['0', '1'], \n",
    "                             Y_noisy=y_noisy)\n",
    "active_test.gen_model_predictions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect labels from vetter\n",
    "\n",
    "Now we are ready to interactively collect labels from the vetter.  To do this, we call `query_vetted`, which has a few options:\n",
    "* `interactive`: set to True to interactively vet labels\n",
    "* `budget`: the number of items to vet\n",
    "* `batch_size`: used by some of the estimators to control how often internal models are retrained.\n",
    "* `raw`: files to display corresponding to items \n",
    "* `visualizer`: function that processes the raw features in X and returns a dictionary of output\n",
    "\n",
    "Only the first two are important at the moment.  Running the cell below will select items and request a label from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning preprocessing to find vetted labels of each class...\n",
      "[-1.72575868 -2.8470473 ]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 0\n",
      "\n",
      "\n",
      "[3.73346014 4.06301183]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "Completed preprocessing\n",
      "Budget reduced from \"10\" to \"8\"\n",
      "[ 0.44015372 -0.08109233]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "[ 2.20254539 -0.65944231]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "[2.29622029 2.09121062]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "[-0.10799278  1.14608362]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "[3.48585406 2.81144642]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "[ 0.17955305 -0.40492947]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 0\n",
      "\n",
      "\n",
      "[ 1.22562061 -0.2740566 ]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "[-0.34345773  0.77914681]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "active_test.query_vetted(interactive=True, budget=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate performance\n",
    "\n",
    "After we are done querying labels from the vetter, we can run the `test()` method to compute our estimated performance and then retrieve the result.  Below, we compare this result with the accuracy we would have estimated, had we not updated the noisy labels.  Running `get_rest_results()` returns a dictionary containing the results.  The indices are \n",
    "* `tester_metric`: estimated value for the performance metric\n",
    "* `tester_labels`: estimated labels for the dataset\n",
    "* `tester_probs`: label probabilities estimated by the Learned estimator (set to None if the Naive estimator is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True accuracy: 0.89\n",
      "Predicted accuracy from active testing: 0.65\n",
      "Predicted accuracy without using active testing: 0.65\n"
     ]
    }
   ],
   "source": [
    "active_test.test()\n",
    "result = active_test.get_test_results()\n",
    "print('True accuracy: '+ str(true_accuracy))\n",
    "print('Predicted accuracy from active testing: '+ str(result['tester_metric']))\n",
    "print('Predicted accuracy without using active testing: ' + str(accuracy_score(y_noisy, label_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional paramters for standardize_data\n",
    "\n",
    "Below, we discuss the full set of parameters for `standardize_data()`:\n",
    "* `rearrange` : boolean value that determines whether to shuffle the dataset.  This is False by default and should be set to False if a list of file names is passed to the raw parameter in `query_vetted()`\n",
    "* `is_img_byte`: boolean value that marks whether the data in `X` can be displayed as an image\n",
    "* `num` : number of samples to draw from the dataset.  This is set to -1, which uses all data.\n",
    "* `X` : array of features (described above)\n",
    "* `classes` : list of class names (described above)\n",
    "* `Y_ground_truth` : known ground truth labels, if available.  This is primarily useful for comparing algorithms when ground truth is known without needing to interactively vet labels.\n",
    "* `Y_vetted` : vetted labels that have already been gathered\n",
    "* `Y_noisy` : the noisy labels (described above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a model when only the predicted probabilities are available\n",
    "\n",
    "If the model itself is not available to pass to `active_tester`, but the probabilities predicted by the model are, we can set these directly instead of calling `gen_model_predictions()`.  The format is expected to be a (number of items) x (number of classes) array containing the predicted probabilties.  Below we show an example where this array is produced by sklearn directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.87510038e-01 1.24899623e-02]\n",
      " [1.73037079e-01 8.26962921e-01]\n",
      " [9.25240749e-01 7.47592513e-02]\n",
      " [1.23432680e-02 9.87656732e-01]\n",
      " [9.99936729e-01 6.32707703e-05]]\n",
      "Beginning preprocessing to find vetted labels of each class...\n",
      "[ 1.03262418 -1.22249529]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 0\n",
      "\n",
      "\n",
      "[ 0.44015372 -0.08109233]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "Completed preprocessing\n",
      "Budget reduced from \"5\" to \"3\"\n",
      "[ 0.47675293 -1.31891426]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 0\n",
      "\n",
      "\n",
      "[2.86072373 1.79840358]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "[2.57358588 0.97990037]\n",
      "The available labels are: ['0', '1']\n",
      "Label the provided item: 1\n",
      "\n",
      "\n",
      "{'tester_labels': array([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1]), 'tester_metric': 0.6, 'tester_prob': None}\n"
     ]
    }
   ],
   "source": [
    "active_test = ActiveTester(Naive(metric=accuracy_score), Random())\n",
    "active_test.standardize_data(X=X_test, \n",
    "                             classes=['0','1'], \n",
    "                             Y_noisy=y_noisy)\n",
    "\n",
    "# run the model on the dataset\n",
    "y_predictions = model.predict_proba(X_test)\n",
    "active_test.set_prob_array(y_predictions)\n",
    "\n",
    "print(y_predictions[:5,:])\n",
    "\n",
    "# query 5 labels from the vetter (you!)\n",
    "active_test.query_vetted(True, budget=5, raw=None, visualizer=None)\n",
    "\n",
    "# use input to estimate classifier performance and print the result\n",
    "active_test.test()\n",
    "result = active_test.get_test_results()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
